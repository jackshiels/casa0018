{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMUOp/ZR/rPYdGPOUXtQG3Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackshiels/casa0018/blob/main/CosWorkshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COS Implementation of the Hello World program"
      ],
      "metadata": {
        "id": "3e-vgVySnYoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wO0dYtYknYCX",
        "outputId": "812fc0c8-9c19-4cc3-b5f6-edae484a25a1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_6 (Dense)             (None, 16)                32        \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 16)                272       \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 321\n",
            "Trainable params: 321\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/600\n",
            "38/38 [==============================] - 1s 9ms/step - loss: 0.5086 - mae: 0.6354 - val_loss: 0.4876 - val_mae: 0.6215\n",
            "Epoch 2/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4940 - mae: 0.6283 - val_loss: 0.4765 - val_mae: 0.6135\n",
            "Epoch 3/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.4831 - mae: 0.6209 - val_loss: 0.4646 - val_mae: 0.6046\n",
            "Epoch 4/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4723 - mae: 0.6135 - val_loss: 0.4526 - val_mae: 0.5951\n",
            "Epoch 5/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4609 - mae: 0.6048 - val_loss: 0.4405 - val_mae: 0.5850\n",
            "Epoch 6/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.4472 - mae: 0.5940 - val_loss: 0.4245 - val_mae: 0.5717\n",
            "Epoch 7/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.4316 - mae: 0.5812 - val_loss: 0.4079 - val_mae: 0.5566\n",
            "Epoch 8/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.4156 - mae: 0.5662 - val_loss: 0.3925 - val_mae: 0.5415\n",
            "Epoch 9/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.3999 - mae: 0.5515 - val_loss: 0.3776 - val_mae: 0.5268\n",
            "Epoch 10/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.3827 - mae: 0.5345 - val_loss: 0.3595 - val_mae: 0.5069\n",
            "Epoch 11/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.3667 - mae: 0.5162 - val_loss: 0.3434 - val_mae: 0.4879\n",
            "Epoch 12/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.3494 - mae: 0.4961 - val_loss: 0.3265 - val_mae: 0.4664\n",
            "Epoch 13/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.3313 - mae: 0.4742 - val_loss: 0.3128 - val_mae: 0.4463\n",
            "Epoch 14/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.3181 - mae: 0.4568 - val_loss: 0.2978 - val_mae: 0.4307\n",
            "Epoch 15/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.2991 - mae: 0.4385 - val_loss: 0.2821 - val_mae: 0.4108\n",
            "Epoch 16/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.2850 - mae: 0.4243 - val_loss: 0.2655 - val_mae: 0.3976\n",
            "Epoch 17/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.2667 - mae: 0.4100 - val_loss: 0.2485 - val_mae: 0.3848\n",
            "Epoch 18/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2488 - mae: 0.3950 - val_loss: 0.2320 - val_mae: 0.3696\n",
            "Epoch 19/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2312 - mae: 0.3796 - val_loss: 0.2143 - val_mae: 0.3565\n",
            "Epoch 20/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.2107 - mae: 0.3620 - val_loss: 0.1950 - val_mae: 0.3365\n",
            "Epoch 21/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.1894 - mae: 0.3423 - val_loss: 0.1746 - val_mae: 0.3203\n",
            "Epoch 22/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.1706 - mae: 0.3251 - val_loss: 0.1572 - val_mae: 0.3021\n",
            "Epoch 23/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1521 - mae: 0.3073 - val_loss: 0.1413 - val_mae: 0.2945\n",
            "Epoch 24/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.1325 - mae: 0.2854 - val_loss: 0.1207 - val_mae: 0.2663\n",
            "Epoch 25/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.1131 - mae: 0.2652 - val_loss: 0.1058 - val_mae: 0.2466\n",
            "Epoch 26/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0926 - mae: 0.2398 - val_loss: 0.0956 - val_mae: 0.2367\n",
            "Epoch 27/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0772 - mae: 0.2181 - val_loss: 0.0710 - val_mae: 0.2067\n",
            "Epoch 28/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0639 - mae: 0.2006 - val_loss: 0.0580 - val_mae: 0.1925\n",
            "Epoch 29/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0504 - mae: 0.1785 - val_loss: 0.0461 - val_mae: 0.1688\n",
            "Epoch 30/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0406 - mae: 0.1590 - val_loss: 0.0361 - val_mae: 0.1507\n",
            "Epoch 31/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0328 - mae: 0.1428 - val_loss: 0.0344 - val_mae: 0.1439\n",
            "Epoch 32/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0268 - mae: 0.1304 - val_loss: 0.0233 - val_mae: 0.1228\n",
            "Epoch 33/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0226 - mae: 0.1208 - val_loss: 0.0219 - val_mae: 0.1173\n",
            "Epoch 34/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0192 - mae: 0.1108 - val_loss: 0.0194 - val_mae: 0.1106\n",
            "Epoch 35/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0179 - mae: 0.1085 - val_loss: 0.0221 - val_mae: 0.1217\n",
            "Epoch 36/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0166 - mae: 0.1037 - val_loss: 0.0143 - val_mae: 0.0953\n",
            "Epoch 37/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0154 - mae: 0.1003 - val_loss: 0.0151 - val_mae: 0.0979\n",
            "Epoch 38/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0152 - mae: 0.0982 - val_loss: 0.0148 - val_mae: 0.0967\n",
            "Epoch 39/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0143 - mae: 0.0953 - val_loss: 0.0127 - val_mae: 0.0914\n",
            "Epoch 40/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0141 - mae: 0.0944 - val_loss: 0.0168 - val_mae: 0.1035\n",
            "Epoch 41/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0139 - mae: 0.0938 - val_loss: 0.0128 - val_mae: 0.0908\n",
            "Epoch 42/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0141 - mae: 0.0935 - val_loss: 0.0130 - val_mae: 0.0916\n",
            "Epoch 43/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0138 - mae: 0.0935 - val_loss: 0.0141 - val_mae: 0.0969\n",
            "Epoch 44/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0133 - mae: 0.0916 - val_loss: 0.0137 - val_mae: 0.0952\n",
            "Epoch 45/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0124 - mae: 0.0906 - val_loss: 0.0131 - val_mae: 0.0917\n",
            "Epoch 46/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0123 - mae: 0.0888 - val_loss: 0.0125 - val_mae: 0.0898\n",
            "Epoch 47/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0121 - mae: 0.0887 - val_loss: 0.0106 - val_mae: 0.0822\n",
            "Epoch 48/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0118 - mae: 0.0875 - val_loss: 0.0114 - val_mae: 0.0858\n",
            "Epoch 49/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0119 - mae: 0.0877 - val_loss: 0.0145 - val_mae: 0.0980\n",
            "Epoch 50/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0121 - mae: 0.0881 - val_loss: 0.0110 - val_mae: 0.0840\n",
            "Epoch 51/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0118 - mae: 0.0864 - val_loss: 0.0113 - val_mae: 0.0861\n",
            "Epoch 52/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0116 - mae: 0.0865 - val_loss: 0.0110 - val_mae: 0.0838\n",
            "Epoch 53/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0116 - mae: 0.0851 - val_loss: 0.0143 - val_mae: 0.0992\n",
            "Epoch 54/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0116 - mae: 0.0866 - val_loss: 0.0114 - val_mae: 0.0836\n",
            "Epoch 55/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0116 - mae: 0.0852 - val_loss: 0.0119 - val_mae: 0.0878\n",
            "Epoch 56/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0855 - val_loss: 0.0109 - val_mae: 0.0820\n",
            "Epoch 57/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0116 - mae: 0.0864 - val_loss: 0.0105 - val_mae: 0.0819\n",
            "Epoch 58/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0856 - val_loss: 0.0114 - val_mae: 0.0854\n",
            "Epoch 59/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0860 - val_loss: 0.0103 - val_mae: 0.0808\n",
            "Epoch 60/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0856 - val_loss: 0.0106 - val_mae: 0.0818\n",
            "Epoch 61/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0855 - val_loss: 0.0105 - val_mae: 0.0824\n",
            "Epoch 62/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0844 - val_loss: 0.0143 - val_mae: 0.0971\n",
            "Epoch 63/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0852 - val_loss: 0.0105 - val_mae: 0.0824\n",
            "Epoch 64/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0833 - val_loss: 0.0207 - val_mae: 0.1206\n",
            "Epoch 65/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0847 - val_loss: 0.0126 - val_mae: 0.0906\n",
            "Epoch 66/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0853 - val_loss: 0.0155 - val_mae: 0.1043\n",
            "Epoch 67/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0844 - val_loss: 0.0125 - val_mae: 0.0921\n",
            "Epoch 68/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0849 - val_loss: 0.0122 - val_mae: 0.0887\n",
            "Epoch 69/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0845 - val_loss: 0.0154 - val_mae: 0.1038\n",
            "Epoch 70/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0840 - val_loss: 0.0137 - val_mae: 0.0940\n",
            "Epoch 71/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0859 - val_loss: 0.0116 - val_mae: 0.0871\n",
            "Epoch 72/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0850 - val_loss: 0.0109 - val_mae: 0.0842\n",
            "Epoch 73/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0838 - val_loss: 0.0136 - val_mae: 0.0936\n",
            "Epoch 74/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0844 - val_loss: 0.0115 - val_mae: 0.0869\n",
            "Epoch 75/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0847 - val_loss: 0.0116 - val_mae: 0.0868\n",
            "Epoch 76/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0838 - val_loss: 0.0107 - val_mae: 0.0830\n",
            "Epoch 77/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0842 - val_loss: 0.0109 - val_mae: 0.0838\n",
            "Epoch 78/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0855 - val_loss: 0.0120 - val_mae: 0.0879\n",
            "Epoch 79/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0831 - val_loss: 0.0112 - val_mae: 0.0834\n",
            "Epoch 80/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0114 - mae: 0.0855 - val_loss: 0.0113 - val_mae: 0.0844\n",
            "Epoch 81/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0843 - val_loss: 0.0104 - val_mae: 0.0812\n",
            "Epoch 82/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0836 - val_loss: 0.0130 - val_mae: 0.0918\n",
            "Epoch 83/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0847 - val_loss: 0.0119 - val_mae: 0.0873\n",
            "Epoch 84/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0136 - val_mae: 0.0932\n",
            "Epoch 85/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0840 - val_loss: 0.0141 - val_mae: 0.0977\n",
            "Epoch 86/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0837 - val_loss: 0.0124 - val_mae: 0.0895\n",
            "Epoch 87/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0838 - val_loss: 0.0117 - val_mae: 0.0865\n",
            "Epoch 88/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0847 - val_loss: 0.0164 - val_mae: 0.1069\n",
            "Epoch 89/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0842 - val_loss: 0.0121 - val_mae: 0.0891\n",
            "Epoch 90/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0847 - val_loss: 0.0107 - val_mae: 0.0828\n",
            "Epoch 91/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0840 - val_loss: 0.0117 - val_mae: 0.0872\n",
            "Epoch 92/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0849 - val_loss: 0.0118 - val_mae: 0.0882\n",
            "Epoch 93/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0844 - val_loss: 0.0114 - val_mae: 0.0853\n",
            "Epoch 94/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0840 - val_loss: 0.0125 - val_mae: 0.0913\n",
            "Epoch 95/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0848 - val_loss: 0.0107 - val_mae: 0.0832\n",
            "Epoch 96/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0855 - val_loss: 0.0102 - val_mae: 0.0803\n",
            "Epoch 97/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0839 - val_loss: 0.0104 - val_mae: 0.0805\n",
            "Epoch 98/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0837 - val_loss: 0.0121 - val_mae: 0.0883\n",
            "Epoch 99/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0838 - val_loss: 0.0106 - val_mae: 0.0826\n",
            "Epoch 100/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0829 - val_loss: 0.0114 - val_mae: 0.0853\n",
            "Epoch 101/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0826 - val_loss: 0.0105 - val_mae: 0.0818\n",
            "Epoch 102/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0834 - val_loss: 0.0116 - val_mae: 0.0874\n",
            "Epoch 103/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0846 - val_loss: 0.0110 - val_mae: 0.0845\n",
            "Epoch 104/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0849 - val_loss: 0.0103 - val_mae: 0.0810\n",
            "Epoch 105/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0840 - val_loss: 0.0103 - val_mae: 0.0808\n",
            "Epoch 106/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0836 - val_loss: 0.0114 - val_mae: 0.0839\n",
            "Epoch 107/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0849 - val_loss: 0.0102 - val_mae: 0.0803\n",
            "Epoch 108/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0852 - val_loss: 0.0121 - val_mae: 0.0884\n",
            "Epoch 109/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0841 - val_loss: 0.0114 - val_mae: 0.0849\n",
            "Epoch 110/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0845 - val_loss: 0.0119 - val_mae: 0.0890\n",
            "Epoch 111/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0835 - val_loss: 0.0111 - val_mae: 0.0847\n",
            "Epoch 112/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0841 - val_loss: 0.0142 - val_mae: 0.0961\n",
            "Epoch 113/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0842 - val_loss: 0.0107 - val_mae: 0.0826\n",
            "Epoch 114/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0115 - mae: 0.0851 - val_loss: 0.0124 - val_mae: 0.0912\n",
            "Epoch 115/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0845 - val_loss: 0.0124 - val_mae: 0.0895\n",
            "Epoch 116/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0851 - val_loss: 0.0136 - val_mae: 0.0953\n",
            "Epoch 117/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0828 - val_loss: 0.0109 - val_mae: 0.0830\n",
            "Epoch 118/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0843 - val_loss: 0.0111 - val_mae: 0.0848\n",
            "Epoch 119/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0847 - val_loss: 0.0109 - val_mae: 0.0844\n",
            "Epoch 120/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0848 - val_loss: 0.0105 - val_mae: 0.0817\n",
            "Epoch 121/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0836 - val_loss: 0.0105 - val_mae: 0.0816\n",
            "Epoch 122/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0841 - val_loss: 0.0104 - val_mae: 0.0812\n",
            "Epoch 123/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0836 - val_loss: 0.0117 - val_mae: 0.0869\n",
            "Epoch 124/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0827 - val_loss: 0.0123 - val_mae: 0.0891\n",
            "Epoch 125/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0834 - val_loss: 0.0107 - val_mae: 0.0827\n",
            "Epoch 126/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0851 - val_loss: 0.0120 - val_mae: 0.0886\n",
            "Epoch 127/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0855 - val_loss: 0.0103 - val_mae: 0.0811\n",
            "Epoch 128/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0843 - val_loss: 0.0103 - val_mae: 0.0806\n",
            "Epoch 129/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0852 - val_loss: 0.0116 - val_mae: 0.0864\n",
            "Epoch 130/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0845 - val_loss: 0.0106 - val_mae: 0.0817\n",
            "Epoch 131/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0825 - val_loss: 0.0103 - val_mae: 0.0803\n",
            "Epoch 132/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0840 - val_loss: 0.0117 - val_mae: 0.0863\n",
            "Epoch 133/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0836 - val_loss: 0.0108 - val_mae: 0.0823\n",
            "Epoch 134/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0844 - val_loss: 0.0108 - val_mae: 0.0822\n",
            "Epoch 135/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0835 - val_loss: 0.0121 - val_mae: 0.0883\n",
            "Epoch 136/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0837 - val_loss: 0.0112 - val_mae: 0.0850\n",
            "Epoch 137/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0835 - val_loss: 0.0105 - val_mae: 0.0813\n",
            "Epoch 138/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0842 - val_loss: 0.0104 - val_mae: 0.0805\n",
            "Epoch 139/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0830 - val_loss: 0.0108 - val_mae: 0.0818\n",
            "Epoch 140/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0842 - val_loss: 0.0104 - val_mae: 0.0817\n",
            "Epoch 141/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0827 - val_loss: 0.0136 - val_mae: 0.0957\n",
            "Epoch 142/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0848 - val_loss: 0.0112 - val_mae: 0.0839\n",
            "Epoch 143/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0844 - val_loss: 0.0143 - val_mae: 0.0967\n",
            "Epoch 144/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0833 - val_loss: 0.0120 - val_mae: 0.0879\n",
            "Epoch 145/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0838 - val_loss: 0.0151 - val_mae: 0.1004\n",
            "Epoch 146/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0842 - val_loss: 0.0127 - val_mae: 0.0891\n",
            "Epoch 147/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0832 - val_loss: 0.0114 - val_mae: 0.0866\n",
            "Epoch 148/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0851 - val_loss: 0.0114 - val_mae: 0.0856\n",
            "Epoch 149/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0848 - val_loss: 0.0176 - val_mae: 0.1074\n",
            "Epoch 150/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0839 - val_loss: 0.0103 - val_mae: 0.0799\n",
            "Epoch 151/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0839 - val_loss: 0.0106 - val_mae: 0.0815\n",
            "Epoch 152/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0843 - val_loss: 0.0121 - val_mae: 0.0891\n",
            "Epoch 153/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0846 - val_loss: 0.0136 - val_mae: 0.0960\n",
            "Epoch 154/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0839 - val_loss: 0.0143 - val_mae: 0.0968\n",
            "Epoch 155/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0844 - val_loss: 0.0118 - val_mae: 0.0857\n",
            "Epoch 156/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0842 - val_loss: 0.0117 - val_mae: 0.0863\n",
            "Epoch 157/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0849 - val_loss: 0.0126 - val_mae: 0.0915\n",
            "Epoch 158/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0843 - val_loss: 0.0117 - val_mae: 0.0871\n",
            "Epoch 159/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0822 - val_loss: 0.0104 - val_mae: 0.0813\n",
            "Epoch 160/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0846 - val_loss: 0.0131 - val_mae: 0.0939\n",
            "Epoch 161/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0842 - val_loss: 0.0115 - val_mae: 0.0854\n",
            "Epoch 162/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0115 - val_mae: 0.0861\n",
            "Epoch 163/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0838 - val_loss: 0.0124 - val_mae: 0.0901\n",
            "Epoch 164/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0824 - val_loss: 0.0110 - val_mae: 0.0831\n",
            "Epoch 165/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0846 - val_loss: 0.0103 - val_mae: 0.0806\n",
            "Epoch 166/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0844 - val_loss: 0.0104 - val_mae: 0.0810\n",
            "Epoch 167/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0103 - val_mae: 0.0798\n",
            "Epoch 168/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0824 - val_loss: 0.0118 - val_mae: 0.0872\n",
            "Epoch 169/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0834 - val_loss: 0.0106 - val_mae: 0.0820\n",
            "Epoch 170/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0854 - val_loss: 0.0101 - val_mae: 0.0796\n",
            "Epoch 171/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0117 - val_mae: 0.0865\n",
            "Epoch 172/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0839 - val_loss: 0.0108 - val_mae: 0.0842\n",
            "Epoch 173/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0846 - val_loss: 0.0102 - val_mae: 0.0803\n",
            "Epoch 174/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0837 - val_loss: 0.0106 - val_mae: 0.0817\n",
            "Epoch 175/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0115 - val_mae: 0.0850\n",
            "Epoch 176/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0841 - val_loss: 0.0115 - val_mae: 0.0850\n",
            "Epoch 177/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0840 - val_loss: 0.0105 - val_mae: 0.0815\n",
            "Epoch 178/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0847 - val_loss: 0.0120 - val_mae: 0.0872\n",
            "Epoch 179/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0823 - val_loss: 0.0104 - val_mae: 0.0817\n",
            "Epoch 180/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0153 - val_mae: 0.1007\n",
            "Epoch 181/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0828 - val_loss: 0.0111 - val_mae: 0.0833\n",
            "Epoch 182/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0112 - mae: 0.0843 - val_loss: 0.0115 - val_mae: 0.0856\n",
            "Epoch 183/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0838 - val_loss: 0.0106 - val_mae: 0.0823\n",
            "Epoch 184/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0839 - val_loss: 0.0101 - val_mae: 0.0799\n",
            "Epoch 185/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0843 - val_loss: 0.0112 - val_mae: 0.0847\n",
            "Epoch 186/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0114 - mae: 0.0844 - val_loss: 0.0114 - val_mae: 0.0860\n",
            "Epoch 187/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0845 - val_loss: 0.0101 - val_mae: 0.0795\n",
            "Epoch 188/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0834 - val_loss: 0.0130 - val_mae: 0.0918\n",
            "Epoch 189/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0833 - val_loss: 0.0104 - val_mae: 0.0815\n",
            "Epoch 190/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0837 - val_loss: 0.0106 - val_mae: 0.0820\n",
            "Epoch 191/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0842 - val_loss: 0.0105 - val_mae: 0.0825\n",
            "Epoch 192/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0830 - val_loss: 0.0113 - val_mae: 0.0854\n",
            "Epoch 193/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0855 - val_loss: 0.0101 - val_mae: 0.0797\n",
            "Epoch 194/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0843 - val_loss: 0.0121 - val_mae: 0.0895\n",
            "Epoch 195/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0839 - val_loss: 0.0105 - val_mae: 0.0811\n",
            "Epoch 196/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0832 - val_loss: 0.0113 - val_mae: 0.0861\n",
            "Epoch 197/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0114 - mae: 0.0845 - val_loss: 0.0106 - val_mae: 0.0819\n",
            "Epoch 198/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0828 - val_loss: 0.0104 - val_mae: 0.0813\n",
            "Epoch 199/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0835 - val_loss: 0.0107 - val_mae: 0.0836\n",
            "Epoch 200/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0831 - val_loss: 0.0117 - val_mae: 0.0882\n",
            "Epoch 201/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0110 - val_mae: 0.0830\n",
            "Epoch 202/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0827 - val_loss: 0.0105 - val_mae: 0.0816\n",
            "Epoch 203/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0851 - val_loss: 0.0104 - val_mae: 0.0810\n",
            "Epoch 204/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0832 - val_loss: 0.0143 - val_mae: 0.0982\n",
            "Epoch 205/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0138 - val_mae: 0.0953\n",
            "Epoch 206/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0122 - val_mae: 0.0883\n",
            "Epoch 207/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0841 - val_loss: 0.0102 - val_mae: 0.0805\n",
            "Epoch 208/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0840 - val_loss: 0.0114 - val_mae: 0.0841\n",
            "Epoch 209/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0837 - val_loss: 0.0103 - val_mae: 0.0802\n",
            "Epoch 210/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0101 - val_mae: 0.0800\n",
            "Epoch 211/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0815 - val_loss: 0.0105 - val_mae: 0.0810\n",
            "Epoch 212/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0119 - val_mae: 0.0879\n",
            "Epoch 213/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0836 - val_loss: 0.0102 - val_mae: 0.0812\n",
            "Epoch 214/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0839 - val_loss: 0.0103 - val_mae: 0.0800\n",
            "Epoch 215/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0109 - val_mae: 0.0844\n",
            "Epoch 216/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0834 - val_loss: 0.0125 - val_mae: 0.0903\n",
            "Epoch 217/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0840 - val_loss: 0.0108 - val_mae: 0.0834\n",
            "Epoch 218/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0837 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 219/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0101 - val_mae: 0.0806\n",
            "Epoch 220/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0831 - val_loss: 0.0120 - val_mae: 0.0873\n",
            "Epoch 221/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0101 - val_mae: 0.0803\n",
            "Epoch 222/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0828 - val_loss: 0.0112 - val_mae: 0.0839\n",
            "Epoch 223/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0838 - val_loss: 0.0103 - val_mae: 0.0802\n",
            "Epoch 224/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0851 - val_loss: 0.0106 - val_mae: 0.0823\n",
            "Epoch 225/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0829 - val_loss: 0.0106 - val_mae: 0.0817\n",
            "Epoch 226/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0840 - val_loss: 0.0113 - val_mae: 0.0855\n",
            "Epoch 227/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0843 - val_loss: 0.0109 - val_mae: 0.0841\n",
            "Epoch 228/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0836 - val_loss: 0.0102 - val_mae: 0.0802\n",
            "Epoch 229/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0112 - val_mae: 0.0848\n",
            "Epoch 230/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0833 - val_loss: 0.0133 - val_mae: 0.0932\n",
            "Epoch 231/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0825 - val_loss: 0.0110 - val_mae: 0.0834\n",
            "Epoch 232/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0832 - val_loss: 0.0104 - val_mae: 0.0806\n",
            "Epoch 233/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0113 - mae: 0.0852 - val_loss: 0.0108 - val_mae: 0.0825\n",
            "Epoch 234/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0836 - val_loss: 0.0103 - val_mae: 0.0809\n",
            "Epoch 235/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0109 - val_mae: 0.0834\n",
            "Epoch 236/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0826 - val_loss: 0.0138 - val_mae: 0.0954\n",
            "Epoch 237/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0829 - val_loss: 0.0104 - val_mae: 0.0813\n",
            "Epoch 238/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0115 - val_mae: 0.0850\n",
            "Epoch 239/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0112 - mae: 0.0847 - val_loss: 0.0112 - val_mae: 0.0846\n",
            "Epoch 240/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0112 - mae: 0.0847 - val_loss: 0.0102 - val_mae: 0.0804\n",
            "Epoch 241/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0838 - val_loss: 0.0131 - val_mae: 0.0921\n",
            "Epoch 242/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0832 - val_loss: 0.0116 - val_mae: 0.0865\n",
            "Epoch 243/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0834 - val_loss: 0.0109 - val_mae: 0.0842\n",
            "Epoch 244/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0825 - val_loss: 0.0115 - val_mae: 0.0857\n",
            "Epoch 245/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0848 - val_loss: 0.0100 - val_mae: 0.0800\n",
            "Epoch 246/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0845 - val_loss: 0.0134 - val_mae: 0.0944\n",
            "Epoch 247/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0837 - val_loss: 0.0101 - val_mae: 0.0797\n",
            "Epoch 248/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0826 - val_loss: 0.0100 - val_mae: 0.0800\n",
            "Epoch 249/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0111 - mae: 0.0843 - val_loss: 0.0103 - val_mae: 0.0809\n",
            "Epoch 250/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0115 - val_mae: 0.0864\n",
            "Epoch 251/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0146 - val_mae: 0.0981\n",
            "Epoch 252/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0830 - val_loss: 0.0099 - val_mae: 0.0796\n",
            "Epoch 253/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0824 - val_loss: 0.0108 - val_mae: 0.0828\n",
            "Epoch 254/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0818 - val_loss: 0.0101 - val_mae: 0.0802\n",
            "Epoch 255/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0832 - val_loss: 0.0126 - val_mae: 0.0905\n",
            "Epoch 256/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0837 - val_loss: 0.0111 - val_mae: 0.0846\n",
            "Epoch 257/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0832 - val_loss: 0.0145 - val_mae: 0.0978\n",
            "Epoch 258/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0828 - val_loss: 0.0119 - val_mae: 0.0877\n",
            "Epoch 259/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0831 - val_loss: 0.0128 - val_mae: 0.0909\n",
            "Epoch 260/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0123 - val_mae: 0.0886\n",
            "Epoch 261/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0831 - val_loss: 0.0141 - val_mae: 0.0974\n",
            "Epoch 262/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0838 - val_loss: 0.0102 - val_mae: 0.0805\n",
            "Epoch 263/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0831 - val_loss: 0.0117 - val_mae: 0.0871\n",
            "Epoch 264/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0834 - val_loss: 0.0099 - val_mae: 0.0790\n",
            "Epoch 265/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0828 - val_loss: 0.0106 - val_mae: 0.0818\n",
            "Epoch 266/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0841 - val_loss: 0.0103 - val_mae: 0.0808\n",
            "Epoch 267/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0099 - val_mae: 0.0795\n",
            "Epoch 268/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0831 - val_loss: 0.0106 - val_mae: 0.0823\n",
            "Epoch 269/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0828 - val_loss: 0.0100 - val_mae: 0.0798\n",
            "Epoch 270/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0834 - val_loss: 0.0099 - val_mae: 0.0791\n",
            "Epoch 271/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0105 - val_mae: 0.0815\n",
            "Epoch 272/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0131 - val_mae: 0.0925\n",
            "Epoch 273/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0114 - val_mae: 0.0856\n",
            "Epoch 274/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0833 - val_loss: 0.0103 - val_mae: 0.0817\n",
            "Epoch 275/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0111 - val_mae: 0.0839\n",
            "Epoch 276/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0122 - val_mae: 0.0886\n",
            "Epoch 277/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0818 - val_loss: 0.0111 - val_mae: 0.0845\n",
            "Epoch 278/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0118 - val_mae: 0.0866\n",
            "Epoch 279/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0841 - val_loss: 0.0104 - val_mae: 0.0814\n",
            "Epoch 280/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0821 - val_loss: 0.0122 - val_mae: 0.0885\n",
            "Epoch 281/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0828 - val_loss: 0.0103 - val_mae: 0.0809\n",
            "Epoch 282/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0832 - val_loss: 0.0109 - val_mae: 0.0831\n",
            "Epoch 283/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0837 - val_loss: 0.0110 - val_mae: 0.0835\n",
            "Epoch 284/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0833 - val_loss: 0.0105 - val_mae: 0.0832\n",
            "Epoch 285/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0111 - mae: 0.0842 - val_loss: 0.0106 - val_mae: 0.0824\n",
            "Epoch 286/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0103 - val_mae: 0.0812\n",
            "Epoch 287/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0834 - val_loss: 0.0111 - val_mae: 0.0845\n",
            "Epoch 288/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0841 - val_loss: 0.0112 - val_mae: 0.0845\n",
            "Epoch 289/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0112 - val_mae: 0.0863\n",
            "Epoch 290/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0823 - val_loss: 0.0113 - val_mae: 0.0847\n",
            "Epoch 291/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0837 - val_loss: 0.0115 - val_mae: 0.0860\n",
            "Epoch 292/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0831 - val_loss: 0.0122 - val_mae: 0.0880\n",
            "Epoch 293/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0110 - mae: 0.0841 - val_loss: 0.0100 - val_mae: 0.0797\n",
            "Epoch 294/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0829 - val_loss: 0.0101 - val_mae: 0.0796\n",
            "Epoch 295/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0834 - val_loss: 0.0112 - val_mae: 0.0843\n",
            "Epoch 296/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0815 - val_loss: 0.0100 - val_mae: 0.0802\n",
            "Epoch 297/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0832 - val_loss: 0.0125 - val_mae: 0.0898\n",
            "Epoch 298/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0101 - val_mae: 0.0805\n",
            "Epoch 299/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0113 - val_mae: 0.0865\n",
            "Epoch 300/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0103 - val_mae: 0.0811\n",
            "Epoch 301/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0814 - val_loss: 0.0105 - val_mae: 0.0806\n",
            "Epoch 302/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0829 - val_loss: 0.0098 - val_mae: 0.0789\n",
            "Epoch 303/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0830 - val_loss: 0.0103 - val_mae: 0.0804\n",
            "Epoch 304/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0130 - val_mae: 0.0910\n",
            "Epoch 305/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0829 - val_loss: 0.0116 - val_mae: 0.0865\n",
            "Epoch 306/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0830 - val_loss: 0.0123 - val_mae: 0.0885\n",
            "Epoch 307/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0826 - val_loss: 0.0126 - val_mae: 0.0905\n",
            "Epoch 308/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0835 - val_loss: 0.0098 - val_mae: 0.0795\n",
            "Epoch 309/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0113 - mae: 0.0855 - val_loss: 0.0099 - val_mae: 0.0793\n",
            "Epoch 310/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0836 - val_loss: 0.0119 - val_mae: 0.0894\n",
            "Epoch 311/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0834 - val_loss: 0.0129 - val_mae: 0.0907\n",
            "Epoch 312/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0836 - val_loss: 0.0107 - val_mae: 0.0824\n",
            "Epoch 313/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0836 - val_loss: 0.0103 - val_mae: 0.0819\n",
            "Epoch 314/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0110 - mae: 0.0835 - val_loss: 0.0116 - val_mae: 0.0869\n",
            "Epoch 315/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0815 - val_loss: 0.0100 - val_mae: 0.0802\n",
            "Epoch 316/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0117 - val_mae: 0.0862\n",
            "Epoch 317/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0829 - val_loss: 0.0115 - val_mae: 0.0872\n",
            "Epoch 318/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0122 - val_mae: 0.0882\n",
            "Epoch 319/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0831 - val_loss: 0.0119 - val_mae: 0.0873\n",
            "Epoch 320/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0099 - val_mae: 0.0801\n",
            "Epoch 321/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0838 - val_loss: 0.0113 - val_mae: 0.0852\n",
            "Epoch 322/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0829 - val_loss: 0.0146 - val_mae: 0.0990\n",
            "Epoch 323/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0836 - val_loss: 0.0112 - val_mae: 0.0846\n",
            "Epoch 324/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0105 - val_mae: 0.0820\n",
            "Epoch 325/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0110 - val_mae: 0.0838\n",
            "Epoch 326/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0815 - val_loss: 0.0106 - val_mae: 0.0822\n",
            "Epoch 327/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0821 - val_loss: 0.0112 - val_mae: 0.0844\n",
            "Epoch 328/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0108 - mae: 0.0837 - val_loss: 0.0101 - val_mae: 0.0804\n",
            "Epoch 329/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0846 - val_loss: 0.0103 - val_mae: 0.0803\n",
            "Epoch 330/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0100 - val_mae: 0.0794\n",
            "Epoch 331/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0824 - val_loss: 0.0104 - val_mae: 0.0814\n",
            "Epoch 332/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0817 - val_loss: 0.0107 - val_mae: 0.0816\n",
            "Epoch 333/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0839 - val_loss: 0.0102 - val_mae: 0.0806\n",
            "Epoch 334/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0837 - val_loss: 0.0121 - val_mae: 0.0882\n",
            "Epoch 335/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0810 - val_loss: 0.0111 - val_mae: 0.0842\n",
            "Epoch 336/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0107 - mae: 0.0828 - val_loss: 0.0137 - val_mae: 0.0947\n",
            "Epoch 337/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0108 - mae: 0.0824 - val_loss: 0.0113 - val_mae: 0.0855\n",
            "Epoch 338/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0108 - mae: 0.0829 - val_loss: 0.0100 - val_mae: 0.0796\n",
            "Epoch 339/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0110 - val_mae: 0.0853\n",
            "Epoch 340/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0099 - val_mae: 0.0799\n",
            "Epoch 341/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0827 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 342/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0819 - val_loss: 0.0109 - val_mae: 0.0830\n",
            "Epoch 343/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0110 - mae: 0.0841 - val_loss: 0.0112 - val_mae: 0.0848\n",
            "Epoch 344/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0115 - val_mae: 0.0848\n",
            "Epoch 345/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0830 - val_loss: 0.0110 - val_mae: 0.0842\n",
            "Epoch 346/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0815 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 347/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0101 - val_mae: 0.0798\n",
            "Epoch 348/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0103 - val_mae: 0.0815\n",
            "Epoch 349/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0827 - val_loss: 0.0104 - val_mae: 0.0811\n",
            "Epoch 350/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0820 - val_loss: 0.0111 - val_mae: 0.0848\n",
            "Epoch 351/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0107 - mae: 0.0821 - val_loss: 0.0102 - val_mae: 0.0813\n",
            "Epoch 352/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0125 - val_mae: 0.0904\n",
            "Epoch 353/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0105 - mae: 0.0819 - val_loss: 0.0100 - val_mae: 0.0802\n",
            "Epoch 354/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0831 - val_loss: 0.0099 - val_mae: 0.0799\n",
            "Epoch 355/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0812 - val_loss: 0.0109 - val_mae: 0.0844\n",
            "Epoch 356/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0826 - val_loss: 0.0106 - val_mae: 0.0831\n",
            "Epoch 357/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0110 - mae: 0.0836 - val_loss: 0.0100 - val_mae: 0.0798\n",
            "Epoch 358/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0105 - mae: 0.0818 - val_loss: 0.0111 - val_mae: 0.0850\n",
            "Epoch 359/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0108 - val_mae: 0.0833\n",
            "Epoch 360/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0108 - val_mae: 0.0834\n",
            "Epoch 361/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0106 - val_mae: 0.0819\n",
            "Epoch 362/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0105 - val_mae: 0.0831\n",
            "Epoch 363/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0827 - val_loss: 0.0109 - val_mae: 0.0832\n",
            "Epoch 364/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0116 - val_mae: 0.0870\n",
            "Epoch 365/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0828 - val_loss: 0.0141 - val_mae: 0.0963\n",
            "Epoch 366/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0837 - val_loss: 0.0098 - val_mae: 0.0797\n",
            "Epoch 367/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0104 - val_mae: 0.0810\n",
            "Epoch 368/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0109 - val_mae: 0.0834\n",
            "Epoch 369/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0109 - val_mae: 0.0829\n",
            "Epoch 370/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0105 - mae: 0.0816 - val_loss: 0.0123 - val_mae: 0.0895\n",
            "Epoch 371/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0111 - val_mae: 0.0848\n",
            "Epoch 372/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0816 - val_loss: 0.0106 - val_mae: 0.0837\n",
            "Epoch 373/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0107 - mae: 0.0828 - val_loss: 0.0101 - val_mae: 0.0810\n",
            "Epoch 374/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0111 - val_mae: 0.0843\n",
            "Epoch 375/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0836 - val_loss: 0.0105 - val_mae: 0.0823\n",
            "Epoch 376/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0824 - val_loss: 0.0113 - val_mae: 0.0853\n",
            "Epoch 377/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0830 - val_loss: 0.0104 - val_mae: 0.0819\n",
            "Epoch 378/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0829 - val_loss: 0.0104 - val_mae: 0.0805\n",
            "Epoch 379/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0824 - val_loss: 0.0103 - val_mae: 0.0803\n",
            "Epoch 380/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0101 - val_mae: 0.0803\n",
            "Epoch 381/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0123 - val_mae: 0.0887\n",
            "Epoch 382/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0823 - val_loss: 0.0121 - val_mae: 0.0882\n",
            "Epoch 383/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0829 - val_loss: 0.0130 - val_mae: 0.0924\n",
            "Epoch 384/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0105 - mae: 0.0820 - val_loss: 0.0115 - val_mae: 0.0869\n",
            "Epoch 385/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0824 - val_loss: 0.0129 - val_mae: 0.0920\n",
            "Epoch 386/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0813 - val_loss: 0.0103 - val_mae: 0.0799\n",
            "Epoch 387/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0104 - mae: 0.0818 - val_loss: 0.0102 - val_mae: 0.0808\n",
            "Epoch 388/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0832 - val_loss: 0.0113 - val_mae: 0.0853\n",
            "Epoch 389/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0827 - val_loss: 0.0111 - val_mae: 0.0843\n",
            "Epoch 390/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0102 - val_mae: 0.0806\n",
            "Epoch 391/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0823 - val_loss: 0.0098 - val_mae: 0.0797\n",
            "Epoch 392/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0829 - val_loss: 0.0100 - val_mae: 0.0799\n",
            "Epoch 393/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0820 - val_loss: 0.0109 - val_mae: 0.0847\n",
            "Epoch 394/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0818 - val_loss: 0.0143 - val_mae: 0.0967\n",
            "Epoch 395/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0102 - val_mae: 0.0805\n",
            "Epoch 396/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0821 - val_loss: 0.0101 - val_mae: 0.0798\n",
            "Epoch 397/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0821 - val_loss: 0.0102 - val_mae: 0.0816\n",
            "Epoch 398/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0104 - val_mae: 0.0816\n",
            "Epoch 399/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0101 - val_mae: 0.0797\n",
            "Epoch 400/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0824 - val_loss: 0.0107 - val_mae: 0.0823\n",
            "Epoch 401/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0831 - val_loss: 0.0101 - val_mae: 0.0804\n",
            "Epoch 402/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0833 - val_loss: 0.0099 - val_mae: 0.0798\n",
            "Epoch 403/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0819 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 404/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0829 - val_loss: 0.0106 - val_mae: 0.0817\n",
            "Epoch 405/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0107 - val_mae: 0.0827\n",
            "Epoch 406/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0101 - val_mae: 0.0814\n",
            "Epoch 407/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0111 - val_mae: 0.0849\n",
            "Epoch 408/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0832 - val_loss: 0.0107 - val_mae: 0.0825\n",
            "Epoch 409/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0832 - val_loss: 0.0101 - val_mae: 0.0798\n",
            "Epoch 410/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0826 - val_loss: 0.0102 - val_mae: 0.0799\n",
            "Epoch 411/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0823 - val_loss: 0.0112 - val_mae: 0.0841\n",
            "Epoch 412/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0830 - val_loss: 0.0121 - val_mae: 0.0881\n",
            "Epoch 413/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0840 - val_loss: 0.0107 - val_mae: 0.0834\n",
            "Epoch 414/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0108 - mae: 0.0834 - val_loss: 0.0102 - val_mae: 0.0811\n",
            "Epoch 415/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0825 - val_loss: 0.0099 - val_mae: 0.0795\n",
            "Epoch 416/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0813 - val_loss: 0.0105 - val_mae: 0.0814\n",
            "Epoch 417/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0100 - val_mae: 0.0805\n",
            "Epoch 418/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0826 - val_loss: 0.0098 - val_mae: 0.0790\n",
            "Epoch 419/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0103 - val_mae: 0.0812\n",
            "Epoch 420/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0833 - val_loss: 0.0103 - val_mae: 0.0803\n",
            "Epoch 421/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0102 - val_mae: 0.0804\n",
            "Epoch 422/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0837 - val_loss: 0.0111 - val_mae: 0.0843\n",
            "Epoch 423/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0819 - val_loss: 0.0128 - val_mae: 0.0916\n",
            "Epoch 424/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0150 - val_mae: 0.0999\n",
            "Epoch 425/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0109 - mae: 0.0835 - val_loss: 0.0099 - val_mae: 0.0798\n",
            "Epoch 426/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0823 - val_loss: 0.0103 - val_mae: 0.0817\n",
            "Epoch 427/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0107 - mae: 0.0835 - val_loss: 0.0100 - val_mae: 0.0798\n",
            "Epoch 428/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0105 - mae: 0.0821 - val_loss: 0.0099 - val_mae: 0.0797\n",
            "Epoch 429/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0833 - val_loss: 0.0102 - val_mae: 0.0809\n",
            "Epoch 430/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0104 - mae: 0.0816 - val_loss: 0.0102 - val_mae: 0.0818\n",
            "Epoch 431/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0820 - val_loss: 0.0116 - val_mae: 0.0866\n",
            "Epoch 432/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0104 - mae: 0.0815 - val_loss: 0.0118 - val_mae: 0.0872\n",
            "Epoch 433/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0105 - mae: 0.0822 - val_loss: 0.0115 - val_mae: 0.0860\n",
            "Epoch 434/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0819 - val_loss: 0.0108 - val_mae: 0.0827\n",
            "Epoch 435/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0824 - val_loss: 0.0104 - val_mae: 0.0815\n",
            "Epoch 436/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0818 - val_loss: 0.0103 - val_mae: 0.0807\n",
            "Epoch 437/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0108 - mae: 0.0828 - val_loss: 0.0097 - val_mae: 0.0789\n",
            "Epoch 438/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0108 - val_mae: 0.0830\n",
            "Epoch 439/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0816 - val_loss: 0.0098 - val_mae: 0.0793\n",
            "Epoch 440/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0820 - val_loss: 0.0097 - val_mae: 0.0789\n",
            "Epoch 441/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0822 - val_loss: 0.0118 - val_mae: 0.0870\n",
            "Epoch 442/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0105 - val_mae: 0.0814\n",
            "Epoch 443/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0105 - val_mae: 0.0823\n",
            "Epoch 444/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0103 - mae: 0.0809 - val_loss: 0.0102 - val_mae: 0.0806\n",
            "Epoch 445/600\n",
            "38/38 [==============================] - 1s 15ms/step - loss: 0.0106 - mae: 0.0826 - val_loss: 0.0102 - val_mae: 0.0807\n",
            "Epoch 446/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0108 - mae: 0.0826 - val_loss: 0.0103 - val_mae: 0.0809\n",
            "Epoch 447/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0107 - val_mae: 0.0828\n",
            "Epoch 448/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0103 - mae: 0.0819 - val_loss: 0.0108 - val_mae: 0.0820\n",
            "Epoch 449/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0107 - mae: 0.0828 - val_loss: 0.0118 - val_mae: 0.0864\n",
            "Epoch 450/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0106 - mae: 0.0828 - val_loss: 0.0108 - val_mae: 0.0826\n",
            "Epoch 451/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0105 - val_mae: 0.0821\n",
            "Epoch 452/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0114 - val_mae: 0.0854\n",
            "Epoch 453/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0105 - mae: 0.0820 - val_loss: 0.0100 - val_mae: 0.0807\n",
            "Epoch 454/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0104 - mae: 0.0810 - val_loss: 0.0103 - val_mae: 0.0808\n",
            "Epoch 455/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0831 - val_loss: 0.0102 - val_mae: 0.0811\n",
            "Epoch 456/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0108 - mae: 0.0821 - val_loss: 0.0109 - val_mae: 0.0827\n",
            "Epoch 457/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0116 - val_mae: 0.0859\n",
            "Epoch 458/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0101 - val_mae: 0.0807\n",
            "Epoch 459/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0813 - val_loss: 0.0135 - val_mae: 0.0936\n",
            "Epoch 460/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0104 - mae: 0.0820 - val_loss: 0.0142 - val_mae: 0.0972\n",
            "Epoch 461/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0104 - mae: 0.0823 - val_loss: 0.0113 - val_mae: 0.0863\n",
            "Epoch 462/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0108 - mae: 0.0834 - val_loss: 0.0120 - val_mae: 0.0878\n",
            "Epoch 463/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0104 - mae: 0.0819 - val_loss: 0.0132 - val_mae: 0.0931\n",
            "Epoch 464/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0111 - val_mae: 0.0843\n",
            "Epoch 465/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0803 - val_loss: 0.0099 - val_mae: 0.0795\n",
            "Epoch 466/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0098 - val_mae: 0.0793\n",
            "Epoch 467/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0813 - val_loss: 0.0104 - val_mae: 0.0811\n",
            "Epoch 468/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0814 - val_loss: 0.0120 - val_mae: 0.0876\n",
            "Epoch 469/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0822 - val_loss: 0.0112 - val_mae: 0.0858\n",
            "Epoch 470/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0829 - val_loss: 0.0107 - val_mae: 0.0818\n",
            "Epoch 471/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0122 - val_mae: 0.0893\n",
            "Epoch 472/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0828 - val_loss: 0.0104 - val_mae: 0.0814\n",
            "Epoch 473/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0108 - val_mae: 0.0844\n",
            "Epoch 474/600\n",
            "38/38 [==============================] - 0s 12ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0111 - val_mae: 0.0841\n",
            "Epoch 475/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0120 - val_mae: 0.0883\n",
            "Epoch 476/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0826 - val_loss: 0.0099 - val_mae: 0.0797\n",
            "Epoch 477/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0833 - val_loss: 0.0104 - val_mae: 0.0813\n",
            "Epoch 478/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0124 - val_mae: 0.0895\n",
            "Epoch 479/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0822 - val_loss: 0.0101 - val_mae: 0.0808\n",
            "Epoch 480/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0802 - val_loss: 0.0119 - val_mae: 0.0871\n",
            "Epoch 481/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0107 - mae: 0.0822 - val_loss: 0.0119 - val_mae: 0.0886\n",
            "Epoch 482/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0148 - val_mae: 0.1006\n",
            "Epoch 483/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0103 - mae: 0.0809 - val_loss: 0.0120 - val_mae: 0.0882\n",
            "Epoch 484/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0098 - val_mae: 0.0794\n",
            "Epoch 485/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0107 - val_mae: 0.0823\n",
            "Epoch 486/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0821 - val_loss: 0.0108 - val_mae: 0.0842\n",
            "Epoch 487/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0100 - val_mae: 0.0798\n",
            "Epoch 488/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0818 - val_loss: 0.0109 - val_mae: 0.0832\n",
            "Epoch 489/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0828 - val_loss: 0.0105 - val_mae: 0.0814\n",
            "Epoch 490/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0832 - val_loss: 0.0100 - val_mae: 0.0801\n",
            "Epoch 491/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0100 - val_mae: 0.0799\n",
            "Epoch 492/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0807 - val_loss: 0.0103 - val_mae: 0.0816\n",
            "Epoch 493/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0819 - val_loss: 0.0097 - val_mae: 0.0791\n",
            "Epoch 494/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0822 - val_loss: 0.0101 - val_mae: 0.0802\n",
            "Epoch 495/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0830 - val_loss: 0.0102 - val_mae: 0.0806\n",
            "Epoch 496/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0821 - val_loss: 0.0109 - val_mae: 0.0832\n",
            "Epoch 497/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0103 - mae: 0.0807 - val_loss: 0.0119 - val_mae: 0.0874\n",
            "Epoch 498/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0143 - val_mae: 0.0974\n",
            "Epoch 499/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0829 - val_loss: 0.0104 - val_mae: 0.0828\n",
            "Epoch 500/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0109 - val_mae: 0.0828\n",
            "Epoch 501/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0104 - val_mae: 0.0831\n",
            "Epoch 502/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0101 - val_mae: 0.0810\n",
            "Epoch 503/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0840 - val_loss: 0.0109 - val_mae: 0.0841\n",
            "Epoch 504/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0103 - mae: 0.0802 - val_loss: 0.0116 - val_mae: 0.0849\n",
            "Epoch 505/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0827 - val_loss: 0.0113 - val_mae: 0.0866\n",
            "Epoch 506/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0827 - val_loss: 0.0122 - val_mae: 0.0884\n",
            "Epoch 507/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0818 - val_loss: 0.0107 - val_mae: 0.0822\n",
            "Epoch 508/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0831 - val_loss: 0.0103 - val_mae: 0.0803\n",
            "Epoch 509/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0108 - mae: 0.0834 - val_loss: 0.0100 - val_mae: 0.0795\n",
            "Epoch 510/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0097 - val_mae: 0.0795\n",
            "Epoch 511/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0098 - val_mae: 0.0792\n",
            "Epoch 512/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0833 - val_loss: 0.0135 - val_mae: 0.0938\n",
            "Epoch 513/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0819 - val_loss: 0.0109 - val_mae: 0.0826\n",
            "Epoch 514/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0109 - mae: 0.0837 - val_loss: 0.0097 - val_mae: 0.0793\n",
            "Epoch 515/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0825 - val_loss: 0.0111 - val_mae: 0.0848\n",
            "Epoch 516/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0828 - val_loss: 0.0102 - val_mae: 0.0799\n",
            "Epoch 517/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0100 - val_mae: 0.0798\n",
            "Epoch 518/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0812 - val_loss: 0.0099 - val_mae: 0.0795\n",
            "Epoch 519/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0822 - val_loss: 0.0130 - val_mae: 0.0915\n",
            "Epoch 520/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0816 - val_loss: 0.0115 - val_mae: 0.0852\n",
            "Epoch 521/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0829 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 522/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0105 - val_mae: 0.0818\n",
            "Epoch 523/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0803 - val_loss: 0.0100 - val_mae: 0.0802\n",
            "Epoch 524/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0832 - val_loss: 0.0105 - val_mae: 0.0824\n",
            "Epoch 525/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0104 - val_mae: 0.0816\n",
            "Epoch 526/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0828 - val_loss: 0.0118 - val_mae: 0.0880\n",
            "Epoch 527/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0109 - mae: 0.0829 - val_loss: 0.0108 - val_mae: 0.0827\n",
            "Epoch 528/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0826 - val_loss: 0.0117 - val_mae: 0.0883\n",
            "Epoch 529/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0097 - val_mae: 0.0797\n",
            "Epoch 530/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0821 - val_loss: 0.0119 - val_mae: 0.0867\n",
            "Epoch 531/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0104 - val_mae: 0.0816\n",
            "Epoch 532/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0818 - val_loss: 0.0098 - val_mae: 0.0799\n",
            "Epoch 533/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0113 - val_mae: 0.0846\n",
            "Epoch 534/600\n",
            "38/38 [==============================] - 0s 10ms/step - loss: 0.0107 - mae: 0.0831 - val_loss: 0.0102 - val_mae: 0.0806\n",
            "Epoch 535/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0827 - val_loss: 0.0104 - val_mae: 0.0818\n",
            "Epoch 536/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0104 - val_mae: 0.0815\n",
            "Epoch 537/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0821 - val_loss: 0.0109 - val_mae: 0.0836\n",
            "Epoch 538/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0116 - val_mae: 0.0862\n",
            "Epoch 539/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0831 - val_loss: 0.0109 - val_mae: 0.0830\n",
            "Epoch 540/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0827 - val_loss: 0.0106 - val_mae: 0.0829\n",
            "Epoch 541/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0820 - val_loss: 0.0101 - val_mae: 0.0803\n",
            "Epoch 542/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0816 - val_loss: 0.0103 - val_mae: 0.0806\n",
            "Epoch 543/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0820 - val_loss: 0.0105 - val_mae: 0.0818\n",
            "Epoch 544/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0828 - val_loss: 0.0096 - val_mae: 0.0793\n",
            "Epoch 545/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0818 - val_loss: 0.0106 - val_mae: 0.0818\n",
            "Epoch 546/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0114 - val_mae: 0.0852\n",
            "Epoch 547/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0103 - mae: 0.0814 - val_loss: 0.0106 - val_mae: 0.0836\n",
            "Epoch 548/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0106 - mae: 0.0814 - val_loss: 0.0098 - val_mae: 0.0795\n",
            "Epoch 549/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0822 - val_loss: 0.0107 - val_mae: 0.0824\n",
            "Epoch 550/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0820 - val_loss: 0.0099 - val_mae: 0.0801\n",
            "Epoch 551/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0120 - val_mae: 0.0882\n",
            "Epoch 552/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0819 - val_loss: 0.0104 - val_mae: 0.0805\n",
            "Epoch 553/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0097 - val_mae: 0.0797\n",
            "Epoch 554/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0816 - val_loss: 0.0104 - val_mae: 0.0810\n",
            "Epoch 555/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0107 - mae: 0.0829 - val_loss: 0.0099 - val_mae: 0.0802\n",
            "Epoch 556/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0116 - val_mae: 0.0861\n",
            "Epoch 557/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0818 - val_loss: 0.0105 - val_mae: 0.0819\n",
            "Epoch 558/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0111 - val_mae: 0.0854\n",
            "Epoch 559/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0122 - val_mae: 0.0889\n",
            "Epoch 560/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0103 - mae: 0.0815 - val_loss: 0.0111 - val_mae: 0.0841\n",
            "Epoch 561/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0111 - val_mae: 0.0846\n",
            "Epoch 562/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0828 - val_loss: 0.0098 - val_mae: 0.0797\n",
            "Epoch 563/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0109 - val_mae: 0.0832\n",
            "Epoch 564/600\n",
            "38/38 [==============================] - 0s 11ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0106 - val_mae: 0.0822\n",
            "Epoch 565/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0814 - val_loss: 0.0101 - val_mae: 0.0803\n",
            "Epoch 566/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0101 - val_mae: 0.0803\n",
            "Epoch 567/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0106 - mae: 0.0815 - val_loss: 0.0104 - val_mae: 0.0809\n",
            "Epoch 568/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0829 - val_loss: 0.0099 - val_mae: 0.0796\n",
            "Epoch 569/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0826 - val_loss: 0.0122 - val_mae: 0.0886\n",
            "Epoch 570/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0832 - val_loss: 0.0110 - val_mae: 0.0843\n",
            "Epoch 571/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0107 - mae: 0.0824 - val_loss: 0.0107 - val_mae: 0.0824\n",
            "Epoch 572/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0822 - val_loss: 0.0101 - val_mae: 0.0815\n",
            "Epoch 573/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0823 - val_loss: 0.0148 - val_mae: 0.0985\n",
            "Epoch 574/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0802 - val_loss: 0.0147 - val_mae: 0.0993\n",
            "Epoch 575/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0825 - val_loss: 0.0112 - val_mae: 0.0844\n",
            "Epoch 576/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0105 - mae: 0.0822 - val_loss: 0.0101 - val_mae: 0.0804\n",
            "Epoch 577/600\n",
            "38/38 [==============================] - 0s 7ms/step - loss: 0.0104 - mae: 0.0828 - val_loss: 0.0118 - val_mae: 0.0868\n",
            "Epoch 578/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0820 - val_loss: 0.0097 - val_mae: 0.0789\n",
            "Epoch 579/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0819 - val_loss: 0.0139 - val_mae: 0.0959\n",
            "Epoch 580/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0104 - mae: 0.0816 - val_loss: 0.0103 - val_mae: 0.0813\n",
            "Epoch 581/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0823 - val_loss: 0.0113 - val_mae: 0.0852\n",
            "Epoch 582/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0105 - mae: 0.0810 - val_loss: 0.0105 - val_mae: 0.0814\n",
            "Epoch 583/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0817 - val_loss: 0.0154 - val_mae: 0.1005\n",
            "Epoch 584/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0106 - mae: 0.0823 - val_loss: 0.0098 - val_mae: 0.0792\n",
            "Epoch 585/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0827 - val_loss: 0.0108 - val_mae: 0.0836\n",
            "Epoch 586/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0108 - mae: 0.0831 - val_loss: 0.0099 - val_mae: 0.0797\n",
            "Epoch 587/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0105 - mae: 0.0824 - val_loss: 0.0116 - val_mae: 0.0859\n",
            "Epoch 588/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0105 - mae: 0.0832 - val_loss: 0.0104 - val_mae: 0.0818\n",
            "Epoch 589/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0810 - val_loss: 0.0101 - val_mae: 0.0811\n",
            "Epoch 590/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0104 - mae: 0.0817 - val_loss: 0.0099 - val_mae: 0.0795\n",
            "Epoch 591/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0102 - mae: 0.0809 - val_loss: 0.0141 - val_mae: 0.0947\n",
            "Epoch 592/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0103 - mae: 0.0820 - val_loss: 0.0100 - val_mae: 0.0794\n",
            "Epoch 593/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0107 - mae: 0.0836 - val_loss: 0.0116 - val_mae: 0.0860\n",
            "Epoch 594/600\n",
            "38/38 [==============================] - 0s 6ms/step - loss: 0.0106 - mae: 0.0829 - val_loss: 0.0103 - val_mae: 0.0811\n",
            "Epoch 595/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0104 - mae: 0.0823 - val_loss: 0.0100 - val_mae: 0.0806\n",
            "Epoch 596/600\n",
            "38/38 [==============================] - 0s 9ms/step - loss: 0.0107 - mae: 0.0831 - val_loss: 0.0112 - val_mae: 0.0843\n",
            "Epoch 597/600\n",
            "38/38 [==============================] - 0s 8ms/step - loss: 0.0103 - mae: 0.0810 - val_loss: 0.0113 - val_mae: 0.0859\n",
            "Epoch 598/600\n",
            "38/38 [==============================] - 0s 4ms/step - loss: 0.0101 - mae: 0.0802 - val_loss: 0.0112 - val_mae: 0.0843\n",
            "Epoch 599/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0825 - val_loss: 0.0105 - val_mae: 0.0830\n",
            "Epoch 600/600\n",
            "38/38 [==============================] - 0s 5ms/step - loss: 0.0106 - mae: 0.0826 - val_loss: 0.0098 - val_mae: 0.0796\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "samples = 1000\n",
        "seed = 2334\n",
        "\n",
        "np.random.seed(seed)\n",
        "tf.random.set_seed(seed)\n",
        "\n",
        "x_values = np.random.uniform(low = 0, high = 2 * math.pi, size = samples)\n",
        "np.random.shuffle(x_values)\n",
        "\n",
        "y_values = np.cos(x_values)\n",
        "y_values += 0.1 * np.random.randn(*y_values.shape)\n",
        "\n",
        "plt.plot(x_values, y_values, 'b.')\n",
        "plt.show()\n",
        "\n",
        "# Training split\n",
        "train_split = int(0.6 * samples)\n",
        "test_split = int(0.2 * samples + train_split)\n",
        "\n",
        "x_train, x_test, x_validate = np.split(x_values, [train_split, test_split])\n",
        "y_train, y_test, y_validate = np.split(y_values, [train_split, test_split])\n",
        "\n",
        "# Model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(16, activation = 'relu', input_shape=(1,)),\n",
        "    tf.keras.layers.Dense(16, activation = 'relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer = 'rmsprop', loss = 'mse', metrics = ['mae'])\n",
        "history = model.fit(x_train, y_train, epochs = 600, batch_size = 16, validation_data = (x_validate, y_validate))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_mae = model.evaluate(x_test, y_test)\n",
        "print(test_loss)\n",
        "print(test_mae)\n",
        "\n",
        "# Create the directory\n",
        "import os\n",
        "MODELS_DIR = 'models/'\n",
        "MODEL_COS_TF = MODELS_DIR + 'model'\n",
        "MODEL_COS_NO_QUANT_TFLITE = MODELS_DIR + 'model_no_quant.tflite'\n",
        "MODEL_COS_TFLITE = MODELS_DIR + 'model.tflite'\n",
        "MODEL_COS_TFLITE_MICRO = MODELS_DIR + 'model.cc'\n",
        "     \n",
        "# Save the model\n",
        "model.save(MODEL_COS_TF)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71MtFLBSqzkb",
        "outputId": "7ad77218-5b7e-4f03-c01e-f9eb67f5fd07"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 3ms/step - loss: 0.0117 - mae: 0.0873\n",
            "0.011699428781867027\n",
            "0.08728564530611038\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert\n",
        "converter = tf.lite.TFLiteConverter.from_saved_model(MODEL_COS_TF)\n",
        "\n",
        "def representative_dataset_generator():\n",
        "  for value in x_train:\n",
        "    yield [np.array(value, dtype = np.float32, ndmin = 2)]\n",
        "\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "# Enforce integer optimisation\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "# Create a data set\n",
        "converter.representative_dataset = representative_dataset_generator\n",
        "\n",
        "model_tflite = converter.convert()\n",
        "\n",
        "# Save\n",
        "open(MODEL_COS_TFLITE, \"wb\").write(model_tflite)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFikwfKCt-4j",
        "outputId": "7ab24024-7a5a-4988-ed27-fdd29def2cfd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2648"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "!xxd -i {MODEL_COS_TFLITE} > {MODEL_COS_TFLITE_MICRO}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-meftpl12U6",
        "outputId": "53cdc153-beee-4fb8-c2b4-3c09f84e9c0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu focal InRelease\n",
            "\r0% [Connecting to security.ubuntu.com] [Connected to cloud.r-project.org (108.1\r                                                                               \rHit:2 http://archive.ubuntu.com/ubuntu focal-updates InRelease\n",
            "Hit:3 https://cloud.r-project.org/bin/linux/ubuntu focal-cran40/ InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu focal-backports InRelease\n",
            "Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:6 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu focal InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu2004/x86_64  Release\n",
            "Hit:9 http://security.ubuntu.com/ubuntu focal-security InRelease\n",
            "Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu focal InRelease\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu focal InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu focal InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the C source file\n",
        "!cat {MODEL_COS_TFLITE_MICRO}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byuz1jxR1-x7",
        "outputId": "350efd3d-9999-4f49-e382-831a401179a9"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "unsigned char models_model_tflite[] = {\n",
            "  0x20, 0x00, 0x00, 0x00, 0x54, 0x46, 0x4c, 0x33, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x20, 0x00, 0x1c, 0x00, 0x18, 0x00, 0x14, 0x00, 0x10, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x04, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x1c, 0x00, 0x00, 0x00, 0x94, 0x00, 0x00, 0x00, 0xec, 0x00, 0x00, 0x00,\n",
            "  0xa8, 0x03, 0x00, 0x00, 0xb8, 0x03, 0x00, 0x00, 0xfc, 0x09, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x0a, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x0a, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00,\n",
            "  0x38, 0x00, 0x00, 0x00, 0x0f, 0x00, 0x00, 0x00, 0x73, 0x65, 0x72, 0x76,\n",
            "  0x69, 0x6e, 0x67, 0x5f, 0x64, 0x65, 0x66, 0x61, 0x75, 0x6c, 0x74, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x94, 0xff, 0xff, 0xff,\n",
            "  0x09, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00,\n",
            "  0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x38, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0xf2, 0xfc, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x0d, 0x00, 0x00, 0x00, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36, 0x5f,\n",
            "  0x69, 0x6e, 0x70, 0x75, 0x74, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x34, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0xdc, 0xff, 0xff, 0xff,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00,\n",
            "  0x43, 0x4f, 0x4e, 0x56, 0x45, 0x52, 0x53, 0x49, 0x4f, 0x4e, 0x5f, 0x4d,\n",
            "  0x45, 0x54, 0x41, 0x44, 0x41, 0x54, 0x41, 0x00, 0x08, 0x00, 0x0c, 0x00,\n",
            "  0x08, 0x00, 0x04, 0x00, 0x08, 0x00, 0x00, 0x00, 0x0b, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x13, 0x00, 0x00, 0x00, 0x6d, 0x69, 0x6e, 0x5f,\n",
            "  0x72, 0x75, 0x6e, 0x74, 0x69, 0x6d, 0x65, 0x5f, 0x76, 0x65, 0x72, 0x73,\n",
            "  0x69, 0x6f, 0x6e, 0x00, 0x0d, 0x00, 0x00, 0x00, 0xb8, 0x02, 0x00, 0x00,\n",
            "  0xb0, 0x02, 0x00, 0x00, 0x90, 0x02, 0x00, 0x00, 0x38, 0x02, 0x00, 0x00,\n",
            "  0x28, 0x01, 0x00, 0x00, 0xd8, 0x00, 0x00, 0x00, 0xb8, 0x00, 0x00, 0x00,\n",
            "  0xa4, 0x00, 0x00, 0x00, 0x9c, 0x00, 0x00, 0x00, 0x94, 0x00, 0x00, 0x00,\n",
            "  0x8c, 0x00, 0x00, 0x00, 0x6c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0xa2, 0xfd, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x58, 0x00, 0x00, 0x00,\n",
            "  0x0c, 0x00, 0x00, 0x00, 0x08, 0x00, 0x0e, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x28, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xeb, 0x03, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x0a, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x0a, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x32, 0x2e, 0x39, 0x2e,\n",
            "  0x32, 0x00, 0x00, 0x00, 0x06, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x31, 0x2e, 0x31, 0x34, 0x2e, 0x30, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x84, 0xfd, 0xff, 0xff,\n",
            "  0x88, 0xfd, 0xff, 0xff, 0x8c, 0xfd, 0xff, 0xff, 0x2e, 0xfe, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x70, 0xee, 0xff, 0xff,\n",
            "  0x3e, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0xe2, 0xd9, 0x46, 0x65, 0x28, 0x2a, 0x30, 0xe4, 0x10, 0x2f, 0x14, 0xf2,\n",
            "  0xe8, 0x2a, 0x81, 0xac, 0x5a, 0xfe, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x77, 0xff, 0xff, 0xff, 0x9d, 0xfd, 0xff, 0xff,\n",
            "  0x21, 0x0e, 0x00, 0x00, 0x98, 0x0f, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x02, 0xee, 0xff, 0xff, 0xb5, 0x0b, 0x00, 0x00, 0xe3, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x07, 0xeb, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x11, 0xff, 0xff, 0xff, 0x0a, 0xff, 0xff, 0xff, 0x70, 0x03, 0x00, 0x00,\n",
            "  0x93, 0xfc, 0xff, 0xff, 0x90, 0x11, 0x00, 0x00, 0xa6, 0xfe, 0xff, 0xff,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0xf1, 0x1e, 0xfe, 0xf1,\n",
            "  0xe6, 0x1c, 0xef, 0x06, 0xea, 0x1e, 0xf8, 0x21, 0xf4, 0x08, 0xe8, 0x08,\n",
            "  0x13, 0x21, 0xe7, 0x1b, 0x12, 0x11, 0xf4, 0xf0, 0xe7, 0x14, 0x04, 0x12,\n",
            "  0xe5, 0xd8, 0x23, 0xef, 0x2e, 0xf0, 0xde, 0xfc, 0xd6, 0xf2, 0x06, 0x1e,\n",
            "  0x07, 0xc8, 0xfc, 0x3d, 0x1f, 0xf6, 0x03, 0x1e, 0x2f, 0xef, 0xdc, 0xbc,\n",
            "  0xc4, 0xda, 0xf6, 0x09, 0xfc, 0xdc, 0xed, 0x07, 0x16, 0x1b, 0x25, 0xfd,\n",
            "  0x13, 0x09, 0x05, 0x1b, 0x22, 0x10, 0xe4, 0x04, 0xfc, 0xf2, 0xdb, 0xeb,\n",
            "  0xf9, 0xd8, 0xec, 0xe9, 0xa0, 0xeb, 0xe9, 0x33, 0x2e, 0xd8, 0x02, 0xed,\n",
            "  0xfa, 0xf4, 0x02, 0xe4, 0xf2, 0x26, 0x22, 0xda, 0x35, 0xef, 0x20, 0xf0,\n",
            "  0xe5, 0xdc, 0xdd, 0x0f, 0x02, 0xe9, 0xf8, 0x23, 0x32, 0xfa, 0x0f, 0x1f,\n",
            "  0x10, 0x1b, 0x06, 0x10, 0x15, 0x24, 0xee, 0xd9, 0x09, 0xe2, 0x1b, 0xe3,\n",
            "  0x01, 0xd8, 0xe4, 0xdf, 0xe1, 0x1f, 0x11, 0x00, 0xf7, 0xee, 0xe0, 0x07,\n",
            "  0x10, 0xdd, 0x1c, 0xf5, 0x05, 0xee, 0xea, 0xe3, 0x9b, 0xf2, 0x0c, 0x13,\n",
            "  0x04, 0x01, 0xf0, 0x13, 0xe7, 0x16, 0xea, 0xcd, 0xf3, 0x2d, 0xdb, 0x17,\n",
            "  0xdc, 0xe8, 0xde, 0xe6, 0xe6, 0x01, 0xeb, 0xf4, 0xf0, 0xea, 0xdf, 0xe0,\n",
            "  0xea, 0x10, 0x06, 0x19, 0xda, 0xff, 0xe2, 0x0b, 0xdb, 0x1c, 0xf9, 0x19,\n",
            "  0x0c, 0xfa, 0xe3, 0xe4, 0x1e, 0x04, 0x04, 0x0e, 0xe6, 0xd8, 0x0c, 0xfe,\n",
            "  0xd7, 0xe8, 0xd7, 0xfc, 0x17, 0x1d, 0xe0, 0x16, 0xdd, 0xf8, 0xed, 0xda,\n",
            "  0x21, 0xde, 0xe0, 0xe1, 0x16, 0xf0, 0xdd, 0xfe, 0xf1, 0xdb, 0xf3, 0x2d,\n",
            "  0xe6, 0xe1, 0x19, 0x0e, 0x81, 0x04, 0xd9, 0x21, 0x25, 0x1d, 0x11, 0x1f,\n",
            "  0xef, 0x09, 0xf0, 0xa9, 0xdd, 0xe1, 0xee, 0x0a, 0x4c, 0x16, 0xe7, 0xec,\n",
            "  0x04, 0xe1, 0xe3, 0x14, 0xf5, 0xf8, 0x01, 0x58, 0x24, 0xf4, 0x0e, 0x17,\n",
            "  0xb2, 0xff, 0xff, 0xff, 0x04, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x1d, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0xf0, 0xff, 0xff, 0x7c, 0xfb, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0xba, 0x02, 0x00, 0x00,\n",
            "  0x75, 0xfc, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x21, 0x15, 0x00, 0x00,\n",
            "  0xbd, 0x18, 0x00, 0x00, 0x10, 0xf5, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0xe7, 0x9e, 0xf9, 0x64, 0x2d, 0x95, 0xfb, 0xcc, 0x1f, 0x49, 0xdb, 0xec,\n",
            "  0xeb, 0x64, 0xe7, 0x81, 0x84, 0xff, 0xff, 0xff, 0x88, 0xff, 0xff, 0xff,\n",
            "  0x0f, 0x00, 0x00, 0x00, 0x4d, 0x4c, 0x49, 0x52, 0x20, 0x43, 0x6f, 0x6e,\n",
            "  0x76, 0x65, 0x72, 0x74, 0x65, 0x64, 0x2e, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00, 0x18, 0x00, 0x14, 0x00,\n",
            "  0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00, 0x0e, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0xdc, 0x00, 0x00, 0x00,\n",
            "  0xe0, 0x00, 0x00, 0x00, 0xe4, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x6d, 0x61, 0x69, 0x6e, 0x00, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x84, 0x00, 0x00, 0x00, 0x3c, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x96, 0xff, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x04, 0x00, 0x04, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x08, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00,\n",
            "  0x06, 0x00, 0x00, 0x00, 0xca, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x08, 0x10, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0xba, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x01, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00,\n",
            "  0x03, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00,\n",
            "  0x16, 0x00, 0x00, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x0b, 0x00, 0x04, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x08,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x1c, 0x00, 0x00, 0x00, 0x00, 0x00, 0x06, 0x00,\n",
            "  0x08, 0x00, 0x07, 0x00, 0x06, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x01,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0xac, 0x04, 0x00, 0x00,\n",
            "  0x2c, 0x04, 0x00, 0x00, 0xa8, 0x03, 0x00, 0x00, 0x3c, 0x03, 0x00, 0x00,\n",
            "  0xc8, 0x02, 0x00, 0x00, 0x5c, 0x02, 0x00, 0x00, 0xe8, 0x01, 0x00, 0x00,\n",
            "  0x34, 0x01, 0x00, 0x00, 0x80, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x90, 0xfb, 0xff, 0xff, 0x18, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x0a, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x54, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x7c, 0xfb, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x7f, 0xec, 0x02, 0x3c, 0x19, 0x00, 0x00, 0x00, 0x53, 0x74, 0x61, 0x74,\n",
            "  0x65, 0x66, 0x75, 0x6c, 0x50, 0x61, 0x72, 0x74, 0x69, 0x74, 0x69, 0x6f,\n",
            "  0x6e, 0x65, 0x64, 0x43, 0x61, 0x6c, 0x6c, 0x3a, 0x30, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x08, 0xfc, 0xff, 0xff, 0x18, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00,\n",
            "  0x40, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09,\n",
            "  0x8c, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x10, 0x00, 0x00, 0x00, 0xf4, 0xfb, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x80, 0xff, 0xff, 0xff,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0xdf, 0x25, 0x01, 0x3c, 0x52, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e,\n",
            "  0x73, 0x65, 0x5f, 0x37, 0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x3b,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x37, 0x2f, 0x52, 0x65, 0x6c,\n",
            "  0x75, 0x3b, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c,\n",
            "  0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x37, 0x2f, 0x42,\n",
            "  0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0xb8, 0xfc, 0xff, 0xff,\n",
            "  0x18, 0x00, 0x00, 0x00, 0x20, 0x00, 0x00, 0x00, 0x40, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x8c, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0xff, 0xff, 0xff, 0xff, 0x10, 0x00, 0x00, 0x00,\n",
            "  0xa4, 0xfc, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x80, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x9b, 0x36, 0x26, 0x3c,\n",
            "  0x52, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36,\n",
            "  0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x3b, 0x73, 0x65, 0x71, 0x75,\n",
            "  0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e,\n",
            "  0x73, 0x65, 0x5f, 0x36, 0x2f, 0x52, 0x65, 0x6c, 0x75, 0x3b, 0x73, 0x65,\n",
            "  0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64,\n",
            "  0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36, 0x2f, 0x42, 0x69, 0x61, 0x73, 0x41,\n",
            "  0x64, 0x64, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0xde, 0xfd, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x07, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02,\n",
            "  0x54, 0x00, 0x00, 0x00, 0x44, 0xfd, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x52, 0xd4, 0xb9, 0x38,\n",
            "  0x2b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x38,\n",
            "  0x2f, 0x42, 0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61,\n",
            "  0x64, 0x56, 0x61, 0x72, 0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x4e, 0xfe, 0xff, 0xff,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x34, 0x00, 0x00, 0x00, 0x06, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x48, 0x00, 0x00, 0x00, 0xb4, 0xfd, 0xff, 0xff,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x79, 0x2d, 0x38, 0x3c, 0x1b, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x38, 0x2f, 0x4d, 0x61, 0x74,\n",
            "  0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0xb6, 0xfe, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x05, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02,\n",
            "  0x54, 0x00, 0x00, 0x00, 0x1c, 0xfe, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0xf7, 0x29, 0xe2, 0x38,\n",
            "  0x2b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x37,\n",
            "  0x2f, 0x42, 0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61,\n",
            "  0x64, 0x56, 0x61, 0x72, 0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x26, 0xff, 0xff, 0xff,\n",
            "  0x14, 0x00, 0x00, 0x00, 0x34, 0x00, 0x00, 0x00, 0x04, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x48, 0x00, 0x00, 0x00, 0x8c, 0xfe, 0xff, 0xff,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0xec, 0x2a, 0x2e, 0x3c, 0x1b, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69, 0x61, 0x6c, 0x5f, 0x32,\n",
            "  0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x37, 0x2f, 0x4d, 0x61, 0x74,\n",
            "  0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x8e, 0xff, 0xff, 0xff, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x30, 0x00, 0x00, 0x00, 0x03, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x02,\n",
            "  0x54, 0x00, 0x00, 0x00, 0xf4, 0xfe, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x11, 0xa6, 0xee, 0x38,\n",
            "  0x2b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36,\n",
            "  0x2f, 0x42, 0x69, 0x61, 0x73, 0x41, 0x64, 0x64, 0x2f, 0x52, 0x65, 0x61,\n",
            "  0x64, 0x56, 0x61, 0x72, 0x69, 0x61, 0x62, 0x6c, 0x65, 0x4f, 0x70, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x10, 0x00, 0x00, 0x00, 0x00, 0x00, 0x0e, 0x00,\n",
            "  0x18, 0x00, 0x14, 0x00, 0x13, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x04, 0x00,\n",
            "  0x0e, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x34, 0x00, 0x00, 0x00,\n",
            "  0x02, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09, 0x48, 0x00, 0x00, 0x00,\n",
            "  0x74, 0xff, 0xff, 0xff, 0x08, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x87, 0x58, 0x97, 0x3b,\n",
            "  0x1b, 0x00, 0x00, 0x00, 0x73, 0x65, 0x71, 0x75, 0x65, 0x6e, 0x74, 0x69,\n",
            "  0x61, 0x6c, 0x5f, 0x32, 0x2f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36,\n",
            "  0x2f, 0x4d, 0x61, 0x74, 0x4d, 0x75, 0x6c, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x14, 0x00, 0x1c, 0x00,\n",
            "  0x18, 0x00, 0x17, 0x00, 0x10, 0x00, 0x0c, 0x00, 0x08, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x04, 0x00, 0x14, 0x00, 0x00, 0x00, 0x18, 0x00, 0x00, 0x00,\n",
            "  0x2c, 0x00, 0x00, 0x00, 0x4c, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x09, 0x64, 0x00, 0x00, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0xff, 0xff, 0xff, 0xff, 0x01, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x0c, 0x00,\n",
            "  0x00, 0x00, 0x00, 0x00, 0x08, 0x00, 0x04, 0x00, 0x0c, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x00, 0x00, 0x14, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x80, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0xff, 0x00, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x04, 0xd6, 0xc9, 0x3c, 0x1f, 0x00, 0x00, 0x00,\n",
            "  0x73, 0x65, 0x72, 0x76, 0x69, 0x6e, 0x67, 0x5f, 0x64, 0x65, 0x66, 0x61,\n",
            "  0x75, 0x6c, 0x74, 0x5f, 0x64, 0x65, 0x6e, 0x73, 0x65, 0x5f, 0x36, 0x5f,\n",
            "  0x69, 0x6e, 0x70, 0x75, 0x74, 0x3a, 0x30, 0x00, 0x02, 0x00, 0x00, 0x00,\n",
            "  0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00, 0x01, 0x00, 0x00, 0x00,\n",
            "  0x10, 0x00, 0x00, 0x00, 0x0c, 0x00, 0x10, 0x00, 0x0f, 0x00, 0x00, 0x00,\n",
            "  0x08, 0x00, 0x04, 0x00, 0x0c, 0x00, 0x00, 0x00, 0x09, 0x00, 0x00, 0x00,\n",
            "  0x04, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x09\n",
            "};\n",
            "unsigned int models_model_tflite_len = 2648;\n"
          ]
        }
      ]
    }
  ]
}